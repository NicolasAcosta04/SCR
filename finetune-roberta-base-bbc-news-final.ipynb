{"metadata":{"kernelspec":{"display_name":"finetune","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1386132,"sourceType":"datasetVersion","datasetId":809083},{"sourceId":10602733,"sourceType":"datasetVersion","datasetId":6562484}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U bitsandbytes transformers tensorboard==2.11 accelerate datasets scipy einops evaluate trl rouge_score hf_xet\n!sudo apt-get install git-lfs --yes","metadata":{"execution":{"iopub.execute_input":"2025-04-30T17:14:35.242386Z","iopub.status.busy":"2025-04-30T17:14:35.241935Z","iopub.status.idle":"2025-04-30T17:14:41.594662Z","shell.execute_reply":"2025-04-30T17:14:41.593692Z","shell.execute_reply.started":"2025-04-30T17:14:35.242366Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom datasets import load_dataset, DatasetDict, Dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    GenerationConfig,\n    RobertaTokenizerFast,\n    RobertaForSequenceClassification,\n    AutoConfig\n)\nfrom tqdm import tqdm\nfrom trl import SFTTrainer\nimport torch\nimport time\nimport pandas as pd\nimport numpy as np\nimport os\nfrom huggingface_hub import notebook_login, HfFolder\n\n# interpreter_login()\nnotebook_login()","metadata":{"execution":{"iopub.execute_input":"2025-04-30T17:18:11.325248Z","iopub.status.busy":"2025-04-30T17:18:11.324714Z","iopub.status.idle":"2025-04-30T17:18:11.339876Z","shell.execute_reply":"2025-04-30T17:18:11.339133Z","shell.execute_reply.started":"2025-04-30T17:18:11.325226Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"060a10e95e044160882064a31faa1545","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Token has not been saved to git credential helper.\n"]}],"execution_count":1},{"cell_type":"markdown","source":"# INTRO\n\nI am finetuning the roberta-base model from FacebookAI. HuggingFace link: http://huggingface.co/FacebookAI/roberta-base\n\nThe finetuned model can be found here (the dataset it was trained on can be found here aswell): https://huggingface.co/nicolasacosta/roberta-base_bbc-news\n\nDataset on HuggingFace: https://huggingface.co/datasets/SetFit/bbc-news\n\nLink to the original dataset (paper is there also): http://mlg.ucd.ie/datasets/bbc.html\n\nThe code has been adapted from the following guide: https://achimoraites.medium.com/fine-tuning-roberta-for-topic-classification-with-hugging-face-transformers-and-datasets-library-c6f8432d0820","metadata":{}},{"cell_type":"markdown","source":"### Setting IDs for model and dataset repositories from HuggingFace","metadata":{}},{"cell_type":"code","source":"model_id = \"roberta-base\"\ndataset_id = \"SetFit/bbc-news\"\n# relace the value with your model: ex <hugging-face-user>/<model-name>\nrepository_id = \"nicolasacosta/roberta-base_bbc-news\"","metadata":{"execution":{"iopub.execute_input":"2025-04-30T17:15:03.083654Z","iopub.status.busy":"2025-04-30T17:15:03.083158Z","iopub.status.idle":"2025-04-30T17:15:03.087519Z","shell.execute_reply":"2025-04-30T17:15:03.086770Z","shell.execute_reply.started":"2025-04-30T17:15:03.083616Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"### Load and adjust dataset ","metadata":{}},{"cell_type":"code","source":"from datasets import ClassLabel\n\nlabel_names = [\"tech\", \"business\", \"sport\", \"entertainment\", \"politics\"]\n\n# Load dataset\ndataset = load_dataset(dataset_id)\n\n# Training and testing datasets\ntrain_dataset = dataset['train']\ntest_dataset = dataset[\"test\"].shard(num_shards=2, index=0)\n\n# Validation dataset\nval_dataset = dataset['test'].shard(num_shards=2, index=1)\n\n# Ensure label column aligns with label_text (in case it's not yet properly aligned)\ndef align_label(example):\n    example[\"label\"] = label_names.index(example[\"label_text\"])\n    return example\n\n# Create the ClassLabel feature\nclass_label = ClassLabel(num_classes=len(label_names), names=label_names)\n\n# Map and cast on each split\ntrain_dataset = train_dataset.map(align_label)\ntrain_dataset = train_dataset.cast_column(\"label\", class_label)\n\ntest_dataset = test_dataset.map(align_label)\ntest_dataset = test_dataset.cast_column(\"label\", class_label)\n\nval_dataset = val_dataset.map(align_label)\nval_dataset = val_dataset.cast_column(\"label\", class_label)\n\nprint(train_dataset.features)","metadata":{"execution":{"iopub.execute_input":"2025-04-30T17:15:05.654499Z","iopub.status.busy":"2025-04-30T17:15:05.653808Z","iopub.status.idle":"2025-04-30T17:15:09.009144Z","shell.execute_reply":"2025-04-30T17:15:09.008370Z","shell.execute_reply.started":"2025-04-30T17:15:05.654467Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['tech', 'business', 'sport', 'entertainment', 'politics'], id=None), 'label_text': Value(dtype='string', id=None)}\n"]}],"execution_count":3},{"cell_type":"markdown","source":"### Preprocess text (data was cleaned by original authors of the dataset)","metadata":{}},{"cell_type":"code","source":"# Preprocessing\ntokenizer = RobertaTokenizerFast.from_pretrained(model_id)\n\n# This function tokenizes the input text using the RoBERTa tokenizer. \n# It applies padding and truncation to ensure that all sequences have the same length (256 tokens).\ndef tokenize(batch):\n    return tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=256)\n\ntrain_dataset = train_dataset.map(tokenize, batched=True, batch_size=len(train_dataset))\nval_dataset = val_dataset.map(tokenize, batched=True, batch_size=len(val_dataset))\ntest_dataset = test_dataset.map(tokenize, batched=True, batch_size=len(test_dataset))","metadata":{"execution":{"iopub.execute_input":"2025-04-30T17:15:11.815718Z","iopub.status.busy":"2025-04-30T17:15:11.815431Z","iopub.status.idle":"2025-04-30T17:15:12.613685Z","shell.execute_reply":"2025-04-30T17:15:12.612966Z","shell.execute_reply.started":"2025-04-30T17:15:11.815696Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3c6c3755a70c4f46b2d6c1c57ab84ed5","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/500 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"execution_count":4},{"cell_type":"code","source":"# Set dataset format\ntrain_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\nval_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\ntest_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])","metadata":{"execution":{"iopub.execute_input":"2025-04-30T17:15:20.252626Z","iopub.status.busy":"2025-04-30T17:15:20.252149Z","iopub.status.idle":"2025-04-30T17:15:20.258129Z","shell.execute_reply":"2025-04-30T17:15:20.257401Z","shell.execute_reply.started":"2025-04-30T17:15:20.252602Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### Add label mapping to Model Config (will make model inference easier)","metadata":{}},{"cell_type":"code","source":"# We will need this to directly output the class names when using the pipeline without mapping the labels later.\n# Extract the number of classes and their names\nnum_labels = train_dataset.features['label'].num_classes\nclass_names = train_dataset.features[\"label\"].names\nprint(f\"number of labels: {num_labels}\")\nprint(f\"the labels: {class_names}\")\n\n# Create an id2label mapping\nid2label = {i: label for i, label in enumerate(class_names)}\nprint(id2label)\n\n# Update the model's configuration with the id2label mapping\nconfig = AutoConfig.from_pretrained(model_id)\nconfig.update({\"id2label\": id2label})","metadata":{"execution":{"iopub.execute_input":"2025-04-30T17:15:22.788871Z","iopub.status.busy":"2025-04-30T17:15:22.788374Z","iopub.status.idle":"2025-04-30T17:15:22.995290Z","shell.execute_reply":"2025-04-30T17:15:22.994728Z","shell.execute_reply.started":"2025-04-30T17:15:22.788847Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["number of labels: 5\n","the labels: ['tech', 'business', 'sport', 'entertainment', 'politics']\n","{0: 'tech', 1: 'business', 2: 'sport', 3: 'entertainment', 4: 'politics'}\n"]}],"execution_count":6},{"cell_type":"markdown","source":"### Helper function for model training metrics","metadata":{}},{"cell_type":"code","source":"def compute_metrics(pred):\n    logits, labels = pred\n    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n    \n    # Calculate all metrics\n    accuracy = accuracy_score(labels, predictions)\n    f1 = f1_score(labels, predictions, average=\"weighted\")\n    precision = precision_score(labels, predictions, average=\"weighted\")\n    recall = recall_score(labels, predictions, average=\"weighted\")\n    \n    return {\n        \"accuracy\": accuracy,\n        \"f1\": f1,\n        \"precision\": precision,\n        \"recall\": recall\n    } ","metadata":{"execution":{"iopub.execute_input":"2025-04-30T17:15:24.882197Z","iopub.status.busy":"2025-04-30T17:15:24.881917Z","iopub.status.idle":"2025-04-30T17:15:24.886706Z","shell.execute_reply":"2025-04-30T17:15:24.886187Z","shell.execute_reply.started":"2025-04-30T17:15:24.882171Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### Load Model with config and define Trainer and training hyperparameters","metadata":{}},{"cell_type":"code","source":"# Model\nmodel = RobertaForSequenceClassification.from_pretrained(model_id, config=config)\n\n# TrainingArguments\ntraining_args = TrainingArguments(\n    output_dir=repository_id,\n    num_train_epochs=5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    eval_strategy=\"epoch\",\n    logging_dir=f\"{repository_id}/logs\",\n    logging_strategy=\"steps\",\n    logging_steps=10,\n    learning_rate=5e-5,\n    weight_decay=0.01,\n    warmup_steps=500,\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    save_total_limit=2,\n    report_to=\"tensorboard\",\n    push_to_hub=True,\n    hub_strategy=\"every_save\",\n    hub_model_id=repository_id,\n    hub_token=HfFolder.get_token(),\n)\n\n# Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics\n)","metadata":{"execution":{"iopub.execute_input":"2025-04-30T17:18:23.931790Z","iopub.status.busy":"2025-04-30T17:18:23.931242Z","iopub.status.idle":"2025-04-30T17:18:24.909959Z","shell.execute_reply":"2025-04-30T17:18:24.909219Z","shell.execute_reply.started":"2025-04-30T17:18:23.931768Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"execution_count":8},{"cell_type":"markdown","source":"### Train the model","metadata":{}},{"cell_type":"code","source":"# Fine-tune the model\ntrainer.train()","metadata":{"execution":{"iopub.execute_input":"2025-04-30T17:20:35.370324Z","iopub.status.busy":"2025-04-30T17:20:35.369625Z","iopub.status.idle":"2025-04-30T17:24:28.018986Z","shell.execute_reply":"2025-04-30T17:24:28.017947Z","shell.execute_reply.started":"2025-04-30T17:20:35.370298Z"},"trusted":true},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='770' max='770' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [770/770 23:38, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.197600</td>\n","      <td>0.155735</td>\n","      <td>0.956000</td>\n","      <td>0.956250</td>\n","      <td>0.958922</td>\n","      <td>0.956000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.266900</td>\n","      <td>0.148644</td>\n","      <td>0.968000</td>\n","      <td>0.967924</td>\n","      <td>0.968387</td>\n","      <td>0.968000</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.018100</td>\n","      <td>0.233200</td>\n","      <td>0.970000</td>\n","      <td>0.969995</td>\n","      <td>0.970477</td>\n","      <td>0.970000</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.095700</td>\n","      <td>0.137774</td>\n","      <td>0.978000</td>\n","      <td>0.978119</td>\n","      <td>0.978484</td>\n","      <td>0.978000</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.000900</td>\n","      <td>0.182590</td>\n","      <td>0.974000</td>\n","      <td>0.974012</td>\n","      <td>0.974281</td>\n","      <td>0.974000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=770, training_loss=0.29517584381528295, metrics={'train_runtime': 1424.1757, 'train_samples_per_second': 4.301, 'train_steps_per_second': 0.541, 'total_flos': 805799311296000.0, 'train_loss': 0.29517584381528295, 'epoch': 5.0})"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"execution_count":9},{"cell_type":"markdown","source":"### Evaluate the model after training","metadata":{}},{"cell_type":"code","source":"# Evaluate the model\ntrainer.evaluate()","metadata":{"execution":{"iopub.execute_input":"2025-04-30T17:24:48.285499Z","iopub.status.busy":"2025-04-30T17:24:48.285238Z","iopub.status.idle":"2025-04-30T17:24:52.356693Z","shell.execute_reply":"2025-04-30T17:24:52.356142Z","shell.execute_reply.started":"2025-04-30T17:24:48.285481Z"},"trusted":true},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [63/63 00:30]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_loss': 0.13777443766593933,\n"," 'eval_accuracy': 0.978,\n"," 'eval_f1': 0.9781194073686014,\n"," 'eval_precision': 0.9784841219575017,\n"," 'eval_recall': 0.978,\n"," 'eval_runtime': 30.6127,\n"," 'eval_samples_per_second': 16.333,\n"," 'eval_steps_per_second': 2.058,\n"," 'epoch': 5.0}"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"execution_count":10},{"cell_type":"markdown","source":"### Save tokenizer and push to HuggingFace repo","metadata":{}},{"cell_type":"code","source":"# Save our tokenizer and create a model card\ntokenizer.save_pretrained(repository_id)\ntrainer.create_model_card()\n# Push the results to the hub\ntrainer.push_to_hub()","metadata":{"execution":{"iopub.execute_input":"2025-04-30T17:24:57.892036Z","iopub.status.busy":"2025-04-30T17:24:57.891445Z","iopub.status.idle":"2025-04-30T17:25:03.606547Z","shell.execute_reply":"2025-04-30T17:25:03.605943Z","shell.execute_reply.started":"2025-04-30T17:24:57.892008Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8fda3758e1c04899b48926bc9fc8c374","version_major":2,"version_minor":0},"text/plain":["events.out.tfevents.1746045283.LAPTOP-LUNKGTUI.27355.1:   0%|          | 0.00/512 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8d34b18016ff4836ba35c24d2700eeff","version_major":2,"version_minor":0},"text/plain":["events.out.tfevents.1746043714.LAPTOP-LUNKGTUI.27355.0:   0%|          | 0.00/24.0k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b7df5b5b6c3d420b84b0e58fedb800a0","version_major":2,"version_minor":0},"text/plain":["Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/nicolasacosta/roberta-base_bbc-news/commit/bf81750a223e0c9d657efc4dc5cba46d6e39f1ae', commit_message='End of training', commit_description='', oid='bf81750a223e0c9d657efc4dc5cba46d6e39f1ae', pr_url=None, repo_url=RepoUrl('https://huggingface.co/nicolasacosta/roberta-base_bbc-news', endpoint='https://huggingface.co', repo_type='model', repo_id='nicolasacosta/roberta-base_bbc-news'), pr_revision=None, pr_num=None)"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"execution_count":11},{"cell_type":"markdown","source":"### Model Inference","metadata":{}},{"cell_type":"code","source":"# TEST MODEL\n\n# from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n\n# Alternative way to conduct model inference, will return the same result\nconfig = AutoConfig.from_pretrained(repository_id)\n\n# Load the pre-trained model and tokenizer\nmodel = AutoModelForSequenceClassification.from_pretrained(repository_id, config=config)\ntokenizer = AutoTokenizer.from_pretrained(repository_id)\n\ntext = \"Traditional TV Outlets Will Begin to Be Rolled Up by Private Equity in 2026, Analyst Predicts  Despite the initial view in Hollywood and on Wall Street that the Trump administration would accelerate consolidation in the media industry, that hasnâ€™t happened. Instead, the president has taken aimâ€¦ [+3241 chars]\"\n        \n# Tokenize and prepare input for model\ninputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n\n# Perform classification\nwith torch.no_grad():\n    outputs = model(**inputs)\n    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n    confidence, predicted_class = torch.max(predictions, dim=1)\n    \n    # Get the predicted label\n    predicted_label = model.config.id2label[predicted_class.item()]\n    confidence = confidence.item()\n\nfrom transformers import pipeline\n\nclassifier = pipeline('text-classification', repository_id)\n\ntext = \"Traditional TV Outlets Will Begin to Be Rolled Up by Private Equity in 2026, Analyst Predicts  Despite the initial view in Hollywood and on Wall Street that the Trump administration would accelerate consolidation in the media industry, that hasnâ€™t happened. Instead, the president has taken aimâ€¦ [+3241 chars]\"\nresult = classifier(text)\n\npredicted_label = result[0][\"label\"]\nprint(f\"Predicted label: {predicted_label}\")\npredicted_score = result[0][\"score\"]\nprint(f\"Predicted score: {predicted_score}\")","metadata":{"execution":{"iopub.execute_input":"2025-04-30T17:31:51.442833Z","iopub.status.busy":"2025-04-30T17:31:51.442060Z","iopub.status.idle":"2025-04-30T17:31:52.466918Z","shell.execute_reply":"2025-04-30T17:31:52.465725Z","shell.execute_reply.started":"2025-04-30T17:31:51.442810Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Device set to use cuda:0\n"]},{"name":"stdout","output_type":"stream","text":["Predicted label: business\n","Predicted score: 0.9955815672874451\n"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n","\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n","\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"execution_count":null}]}